{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e5f091d-cd95-4790-94ec-28014551c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  \n",
    "from langchain.vectorstores import Chroma \n",
    "import gradio as gr \n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4faa36ef-3d2d-49d4-b9f9-e26b25a63885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm():\n",
    "    llm=ChatGroq(temperature=0, groq_api_key=\"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\",model_name=\"llama-3.3-70b-versatile\")\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4255fc0-f808-49ca-b35f-c75d9f0da311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db():\n",
    "    loader=DirectoryLoader(\"data\",glob=\"*.pdf\",loader_cls=PyPDFLoader)\n",
    "    documents=loader.load()\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "    texts=text_splitter.split_documents(documents)\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    vector_db=Chroma.from_documents(texts,embeddings,persist_directory='./chroma_db')\n",
    "    vector_db.persist()\n",
    "\n",
    "    print(\"ChromaDB created and data saved\")\n",
    "\n",
    "    return vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32e0905c-93f4-4e93-874b-896190119a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_qa_chain(vector_db, llm):\n",
    "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})  # Retrieve top 3 relevant results\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "        output_key=\"answer\"  \n",
    "    )\n",
    "\n",
    "    # Chatbot Prompt\n",
    "    prompt_template = \"\"\"\n",
    "    You are **Medica**, an AI-powered healthcare assistant **integrated into a web application** that helps users track their health and assess risks for conditions like **heart disease, diabetes, and other health concerns**.  \n",
    "    \n",
    "    üñ•Ô∏è **Medica‚Äôs Role in the Web App**:  \n",
    "    ‚úÖ **Web App First Approach**: Always guide users to use the **web app for tests, diagnosis, and specialist recommendations**.  \n",
    "    ‚úÖ **Prompt Users to Take Tests**: If symptoms are mentioned, instruct users to take the **required medical tests (blood test, sugar test, blood pressure check, etc.)** and **enter results in the web app** for diagnosis.  \n",
    "    ‚úÖ **Direct Diagnosis in Chat when necessary**: Do **not attempt to diagnose in chat unless asked for help**‚Äîinstead, direct users to **input test results in the app** to get an accurate ML-based assessment.  \n",
    "    ‚úÖ **Health Risk Predictions via ML**: Explain that the **ML model in the web app analyzes test data** to assess potential health risks.  \n",
    "    ‚úÖ **Find Nearby Doctors via App**: Encourage users to use the app's **specialist locator feature** to find nearby healthcare providers.  \n",
    "    ‚úÖ **Keep Responses Focused**: Do not engage in long medical discussions‚Äîalways steer users back to the app for proper evaluation.  \n",
    "    \n",
    "    ---  \n",
    "    üè• **User‚Äôs Available Health Data**: {context}  \n",
    "    ‚ùì **User‚Äôs Question / Symptoms**: {question}  \n",
    "\n",
    "    üí° **Medica‚Äôs Response**:  \n",
    "    \"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=['question', 'context'])\n",
    "\n",
    "    # QA Chain with retrieval-augmented responses\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        return_source_documents=True,\n",
    "        combine_docs_chain_kwargs={\"prompt\": PROMPT}  \n",
    "    )\n",
    "\n",
    "    return qa_chain  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2685080e-057f-4bcc-982d-7fc2c80454d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chatbot......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANWESA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\components\\chatbot.py:288: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Initializing Chatbot......\") \n",
    "llm = initialize_llm()  # ‚úÖ Ensure this function exists\n",
    "db_path = \"chroma_db/\"\n",
    "\n",
    "\n",
    "if not os.path.exists(db_path):  \n",
    "    vector_db = create_vector_db()  # ‚úÖ Create DB if it doesn't exist\n",
    "else:\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
    "\n",
    "qa_chain = setup_qa_chain(vector_db, llm)  # ‚úÖ Ensure this function exists\n",
    "\n",
    "# while True:\n",
    "#     query = input(\"\\nHuman: \")\n",
    "#     if query.lower() == \"exit\":\n",
    "#         print(\"Aryabhatta: Take Care Brotha, Tada!\")\n",
    "#         break\n",
    "\n",
    "  \n",
    "#     # ‚úÖ Ensure proper dictionary structure for LangChain\n",
    "#     response = qa_chain({\"question\": query, \"chat_history\": []})\n",
    "\n",
    "#     # ‚úÖ Extract the \"answer\" field correctly\n",
    "#     if 'answer' in response:\n",
    "#         print(f\"Aryabhatta: {response['answer']}\")\n",
    "#     else:\n",
    "#         print(\"Aryabhatta: Sorry, I couldn't process that.\")\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\": \n",
    "#      main()\n",
    "\n",
    "\n",
    "def chatbot_response(user_input, history):\n",
    "    if not user_input.strip():\n",
    "        return \"Please provide a valid input\"\n",
    "    \n",
    "    response = qa_chain({\"question\": user_input, \"chat_history\": history})\n",
    "    answer = response.get(\"answer\", \"I couldn't process that. Please consult a healthcare professional. üè•\")\n",
    "\n",
    "    return answer  # ‚úÖ No manual history append\n",
    "\n",
    "with gr.Blocks(theme='JohnSmith9982/small_and_pretty') as app:\n",
    "    chatbot=gr.ChatInterface(fn=chatbot_response,title=\"Medica - AI Health Assistant\")\n",
    "\n",
    "app.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb5116-6360-4209-b7df-09968d75be9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
